ğŸ“ DescriÃ§Ã£o
Script de Web Scraping em Python para automaÃ§Ã£o de coleta de textos no portal CIPM (Corpus Informatizado do PortuguÃªs Medieval). O script acessa pÃ¡ginas protegidas por login, navega por links de categorias e subcategorias, extrai o conteÃºdo textual das pÃ¡ginas e salva localmente em arquivos .txt organizados em pastas.

ğŸš€ Tecnologias utilizadas
Python 3

Selenium â€“ automaÃ§Ã£o de navegaÃ§Ã£o web

BeautifulSoup â€“ extraÃ§Ã£o de conteÃºdo HTML

ChromeDriver â€“ driver do navegador Chrome

ğŸ“¡ API
âŒ Este projeto nÃ£o possui API.
Ã‰ um script de automaÃ§Ã£o de navegaÃ§Ã£o web (web scraping) usando Selenium.

ğŸ’» Requisitos para rodar
Python instalado (versÃ£o 3.x)

Instalar as bibliotecas necessÃ¡rias:
```
bash
pip install selenium
pip install beautifulsoup4
```

Chrome instalado na mÃ¡quina

ChromeDriver compatÃ­vel com a sua versÃ£o do Chrome
Baixe em: https://sites.google.com/chromium.org/driver/

Configurar o caminho do ChromeDriver no cÃ³digo:

```
python
service = Service('/caminho/para/o/chromedriver')
```

Alterar o trecho do login no cÃ³digo para suas credenciais:

```
url = "https://usuario:senha@cipm.fcsh.unl.pt/login"
```

âš™ï¸ Como executar
Clone o repositÃ³rio:

```
bash
git clone https://github.com/seu-usuario/nome-do-repositorio.git
```

Instale as dependÃªncias:

```
bash
pip install selenium beautifulsoup4
```

Edite o caminho do ChromeDriver e as credenciais no cÃ³digo.

Execute o script:

```
bash
python robo_coleta_textos.py
```

Os arquivos .txt serÃ£o salvos em pastas organizadas pelo nome das categorias e subcategorias.
